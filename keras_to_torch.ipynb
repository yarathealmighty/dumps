{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMagN6eojaI3bdr2iWWfL6x"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh4PsRnpJIeT",
        "outputId": "45d26cce-625c-47ad-a354-d6c689721b68"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchinfo\n",
        "from torchsummary import summary\n",
        "\n",
        "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "import nltk"
      ],
      "metadata": {
        "id": "3VwihK3BFuap"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    tokens = text.split()\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    tokens = [re_punc.sub('', w) for w in tokens]\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if w.lower() not in stop_words]\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return tokens\n",
        "\n",
        "def build_vocab_from_df(df, text_column):\n",
        "    vocab = Counter()\n",
        "    for text in df[text_column]:\n",
        "        tokens = clean_text(text)\n",
        "        vocab.update(tokens)\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "VPins8BYOzXb"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SBB2_OLCMyN",
        "outputId": "9181d627-8c3e-4ec5-da6a-c4f4965b4543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                   [-1, 10]             110\n",
            "            Linear-2                   [-1, 20]             220\n",
            "            Linear-3                   [-1, 10]             210\n",
            "            Linear-4                    [-1, 1]              11\n",
            "================================================================\n",
            "Total params: 551\n",
            "Trainable params: 551\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.lin1 = nn.Linear(10, 10)\n",
        "        self.lin2 = nn.Linear(10, 20)\n",
        "        self.lin3 = nn.Linear(20, 10)\n",
        "        self.output = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        h1 = F.relu(self.lin1(input))\n",
        "        h2 = F.relu(self.lin2(h1))\n",
        "        h3 = F.relu(self.lin3(h2))\n",
        "        output = torch.sigmoid(self.output(h3))\n",
        "        return output\n",
        "\n",
        "model = MLP()\n",
        "\n",
        "summary(model, input_size=(10,))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.c2p1 = nn.Conv2d(1, 32, kernel_size=4)\n",
        "        self.p2c1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.c2p2 = nn.Conv2d(32, 16, kernel_size=4)\n",
        "        self.p2lin = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.lin1 = nn.Linear(16 * 13 * 13, 10)\n",
        "        self.output = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        c1 = F.relu(self.c2p1(input))\n",
        "        p1 = self.p2c1(c1)\n",
        "        c2 = F.relu(self.c2p2(p1))\n",
        "        p2 = self.p2lin(c2)\n",
        "        p2_flat = p2.view(-1, 16 * 13 * 13)\n",
        "        h1 = F.relu(self.lin1(p2_flat))\n",
        "        output = torch.sigmoid(self.output(h1))\n",
        "        return output\n",
        "\n",
        "model = CNN()\n",
        "\n",
        "summary(model, input_size=(1, 64, 64))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfYIio_vC12Z",
        "outputId": "2a3bacb0-35e7-4619-8678-82cc78461f59"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 61, 61]             544\n",
            "         MaxPool2d-2           [-1, 32, 30, 30]               0\n",
            "            Conv2d-3           [-1, 16, 27, 27]           8,208\n",
            "         MaxPool2d-4           [-1, 16, 13, 13]               0\n",
            "            Linear-5                   [-1, 10]          27,050\n",
            "            Linear-6                    [-1, 1]              11\n",
            "================================================================\n",
            "Total params: 35,813\n",
            "Trainable params: 35,813\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.02\n",
            "Forward/backward pass size (MB): 1.24\n",
            "Params size (MB): 0.14\n",
            "Estimated Total Size (MB): 1.39\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNN, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=10, batch_first=True)\n",
        "        self.lin1 = nn.Linear(10, 10)\n",
        "        self.output = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        lstm_out, _ = self.lstm(input)\n",
        "        last_hidden_state = lstm_out[:, -1, :]\n",
        "        h1 = F.relu(self.lin1(last_hidden_state))\n",
        "        output = torch.sigmoid(self.output(h1))\n",
        "        return output\n",
        "\n",
        "model = RNN()\n",
        "\n",
        "#LSTM-el meggyűlt a baja a torchsummary-nak\n",
        "torchinfo.summary(model, input_size=(1,100, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNxvq2RWHMqa",
        "outputId": "c884440d-50d8-49d7-9306-72befc56afbb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "RNN                                      [1, 1]                    --\n",
              "├─LSTM: 1-1                              [1, 100, 10]              520\n",
              "├─Linear: 1-2                            [1, 10]                   110\n",
              "├─Linear: 1-3                            [1, 1]                    11\n",
              "==========================================================================================\n",
              "Total params: 641\n",
              "Trainable params: 641\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.05\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.01\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.01\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "result = tokenizer(text)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-qaGyESIjzD",
        "outputId": "69113b7e-5668-4354-e32d-3c5c8358b9f6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://github.com/yarathealmighty/dumps/blob/main/data/not_na_tags/latin_374_w2v.csv?raw=true')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGJw40BUK7iC",
        "outputId": "da2bcb62-9f2e-48d5-d23e-56dcc89525bd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_vocab_from_df(df, 'Question')\n",
        "\n",
        "print('Vocabulary Size:', len(vocab))\n",
        "print('Top Words:', vocab.most_common(50))\n",
        "\n",
        "min_occurane = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTkwJwFHL7bS",
        "outputId": "b581bf74-8750-4ed7-e31c-52d09dfeb109"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 57518\n",
            "Top Words: [('Latin', 7672), ('would', 3867), ('like', 2709), ('word', 2621), ('one', 2091), ('translation', 1862), ('Im', 1857), ('English', 1745), ('words', 1647), ('used', 1628), ('know', 1560), ('Greek', 1519), ('use', 1514), ('question', 1495), ('meaning', 1421), ('say', 1414), ('something', 1364), ('et', 1317), ('also', 1216), ('mean', 1178), ('verb', 1176), ('could', 1156), ('seems', 1147), ('two', 1144), ('est', 1141), ('phrase', 1140), ('sentence', 1124), ('example', 1113), ('translate', 1058), ('way', 1050), ('first', 1015), ('correct', 1008), ('find', 950), ('see', 836), ('sense', 826), ('dont', 813), ('form', 807), ('case', 800), ('found', 793), ('think', 791), ('means', 790), ('make', 778), ('different', 763), ('following', 722), ('name', 704), ('want', 689), ('noun', 686), ('good', 678), ('might', 672), ('understand', 663)]\n",
            "25297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JVX_vxQSOHG0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}