{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXX/8oC/s7bZhh9+8kuK7Y"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh4PsRnpJIeT",
        "outputId": "1dacc073-d986-4ad1-a485-515b5cea6f35"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchinfo\n",
        "from torchsummary import summary\n",
        "\n",
        "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "import nltk\n",
        "\n",
        "import torch.optim as optim\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from collections import OrderedDict\n",
        "from torchtext.transforms import VocabTransform\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "3VwihK3BFuap"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    tokens = text.split()\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    tokens = [re_punc.sub('', w) for w in tokens]\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if w.lower() not in stop_words]\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return tokens\n",
        "\n",
        "def build_vocab_from_df(df, text_column):\n",
        "    vocab = Counter()\n",
        "    for text in df[text_column]:\n",
        "        tokens = clean_text(text)\n",
        "        vocab.update(tokens)\n",
        "    return vocab\n",
        "\n",
        "def pad_sequences(sequences, maxlen, padding_value=0):\n",
        "    padded = [seq + [padding_value] * (maxlen - len(seq)) if len(seq) < maxlen else seq[:maxlen] for seq in sequences]\n",
        "    return torch.tensor(padded, dtype=torch.long).clone().detach()\n",
        "\n",
        "def train_model(model, dataloader, criterion, optimizer, epochs=50):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for batch_docs, batch_labels in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_docs).squeeze()\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_docs, batch_labels in dataloader:\n",
        "            outputs = model(batch_docs).squeeze()\n",
        "            predicted = outputs.round()\n",
        "            correct += (predicted == batch_labels).sum().item()\n",
        "            total += batch_labels.size(0)\n",
        "        accuracy = correct / total\n",
        "        return accuracy\n",
        "\n",
        "def yield_tokens(data):\n",
        "    for doc in data:\n",
        "        yield tokenizer(doc)"
      ],
      "metadata": {
        "id": "VPins8BYOzXb"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SBB2_OLCMyN",
        "outputId": "ae474b93-db00-4898-cb76-80f83ecf615e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                   [-1, 10]             110\n",
            "            Linear-2                   [-1, 20]             220\n",
            "            Linear-3                   [-1, 10]             210\n",
            "            Linear-4                    [-1, 1]              11\n",
            "================================================================\n",
            "Total params: 551\n",
            "Trainable params: 551\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.lin1 = nn.Linear(10, 10)\n",
        "        self.lin2 = nn.Linear(10, 20)\n",
        "        self.lin3 = nn.Linear(20, 10)\n",
        "        self.output = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        h1 = F.relu(self.lin1(input))\n",
        "        h2 = F.relu(self.lin2(h1))\n",
        "        h3 = F.relu(self.lin3(h2))\n",
        "        output = torch.sigmoid(self.output(h3))\n",
        "        return output\n",
        "\n",
        "model = MLP()\n",
        "\n",
        "summary(model, input_size=(10,))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.c2p1 = nn.Conv2d(1, 32, kernel_size=4)\n",
        "        self.p2c1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.c2p2 = nn.Conv2d(32, 16, kernel_size=4)\n",
        "        self.p2lin = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.lin1 = nn.Linear(16 * 13 * 13, 10)\n",
        "        self.output = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        c1 = F.relu(self.c2p1(input))\n",
        "        p1 = self.p2c1(c1)\n",
        "        c2 = F.relu(self.c2p2(p1))\n",
        "        p2 = self.p2lin(c2)\n",
        "        p2_flat = p2.view(-1, 16 * 13 * 13)\n",
        "        h1 = F.relu(self.lin1(p2_flat))\n",
        "        output = torch.sigmoid(self.output(h1))\n",
        "        return output\n",
        "\n",
        "model = CNN()\n",
        "\n",
        "summary(model, input_size=(1, 64, 64))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfYIio_vC12Z",
        "outputId": "cc79a1fa-5ba4-4b22-98c5-e08bcc45d7cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 61, 61]             544\n",
            "         MaxPool2d-2           [-1, 32, 30, 30]               0\n",
            "            Conv2d-3           [-1, 16, 27, 27]           8,208\n",
            "         MaxPool2d-4           [-1, 16, 13, 13]               0\n",
            "            Linear-5                   [-1, 10]          27,050\n",
            "            Linear-6                    [-1, 1]              11\n",
            "================================================================\n",
            "Total params: 35,813\n",
            "Trainable params: 35,813\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.02\n",
            "Forward/backward pass size (MB): 1.24\n",
            "Params size (MB): 0.14\n",
            "Estimated Total Size (MB): 1.39\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNN, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=10, batch_first=True)\n",
        "        self.lin1 = nn.Linear(10, 10)\n",
        "        self.output = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        lstm_out, _ = self.lstm(input)\n",
        "        last_hidden_state = lstm_out[:, -1, :]\n",
        "        h1 = F.relu(self.lin1(last_hidden_state))\n",
        "        output = torch.sigmoid(self.output(h1))\n",
        "        return output\n",
        "\n",
        "model = RNN()\n",
        "\n",
        "#LSTM-el meggyűlt a baja a torchsummary-nak\n",
        "torchinfo.summary(model, input_size=(1,100, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNxvq2RWHMqa",
        "outputId": "b31284d7-d6c1-4ff4-9687-0941ffae3902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "RNN                                      [1, 1]                    --\n",
              "├─LSTM: 1-1                              [1, 100, 10]              520\n",
              "├─Linear: 1-2                            [1, 10]                   110\n",
              "├─Linear: 1-3                            [1, 1]                    11\n",
              "==========================================================================================\n",
              "Total params: 641\n",
              "Trainable params: 641\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.05\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.01\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.01\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "result = tokenizer(text)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-qaGyESIjzD",
        "outputId": "326e84ea-f1cd-48f5-890f-8b2ee96fde96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://github.com/yarathealmighty/dumps/blob/main/data/not_na_tags/latin_374_w2v.csv?raw=true')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGJw40BUK7iC",
        "outputId": "4ee2f205-5d30-4432-f487-cc7a5cdc3ac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_vocab_from_df(df, 'Question')\n",
        "\n",
        "print('Vocabulary Size:', len(vocab))\n",
        "print('Top Words:', vocab.most_common(50))\n",
        "\n",
        "min_occurane = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTkwJwFHL7bS",
        "outputId": "ebeb70e4-6711-4628-c1e4-54ac6fc57d7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 57518\n",
            "Top Words: [('Latin', 7672), ('would', 3867), ('like', 2709), ('word', 2621), ('one', 2091), ('translation', 1862), ('Im', 1857), ('English', 1745), ('words', 1647), ('used', 1628), ('know', 1560), ('Greek', 1519), ('use', 1514), ('question', 1495), ('meaning', 1421), ('say', 1414), ('something', 1364), ('et', 1317), ('also', 1216), ('mean', 1178), ('verb', 1176), ('could', 1156), ('seems', 1147), ('two', 1144), ('est', 1141), ('phrase', 1140), ('sentence', 1124), ('example', 1113), ('translate', 1058), ('way', 1050), ('first', 1015), ('correct', 1008), ('find', 950), ('see', 836), ('sense', 826), ('dont', 813), ('form', 807), ('case', 800), ('found', 793), ('think', 791), ('means', 790), ('make', 778), ('different', 763), ('following', 722), ('name', 704), ('want', 689), ('noun', 686), ('good', 678), ('might', 672), ('understand', 663)]\n",
            "25297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, max_length):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.embed = torch.nn.Embedding(vocab_size,embed_size,padding_idx=vocab['<pad>'])\n",
        "        self.flatten = torch.nn.Flatten()\n",
        "        self.lin1 = nn.Linear(max_length * embed_size, 1)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.embed(input)\n",
        "        flattened = self.flatten(embedded)\n",
        "        h1 = self.lin1(flattened)\n",
        "        output = self.sigmoid(h1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "UZaF5legEsEm"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = ['Well done!',\n",
        "        'Good work',\n",
        "        'Great effort',\n",
        "        'nice work',\n",
        "        'Excellent!',\n",
        "        'Weak',\n",
        "        'Poor effort!',\n",
        "        'not good',\n",
        "        'poor work',\n",
        "        'Could have done better.']\n",
        "\n",
        "labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
        "\n",
        "embed_size = 8\n",
        "max_length = 4\n",
        "num_epochs = 50"
      ],
      "metadata": {
        "id": "pezWB3MHFDR1"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# vocab\n",
        "vocab = build_vocab_from_iterator(yield_tokens(docs), specials=['<unk>', '<pad>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "# encoding\n",
        "encoded_docs = [vocab(tokenizer(doc)) for doc in docs]\n",
        "print(encoded_docs,'\\n')\n",
        "\n",
        "# padding, might try padding layer in NN\n",
        "padded_docs = pad_sequence([torch.tensor(doc) for doc in encoded_docs], batch_first=True, padding_value=vocab['<pad>'])\n",
        "padded_docs = padded_docs[:, :max_length]\n",
        "print(padded_docs,'\\n')\n",
        "\n",
        "if isinstance(labels,list):\n",
        "  labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "dataset = TensorDataset(padded_docs, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "model = SimpleNN(vocab_size=len(vocab), embed_size=embed_size, max_length=max_length)\n",
        "\n",
        "input_tensor = torch.randint(0, len(vocab), (1, max_length))\n",
        "print(torchinfo.summary(model, input_data=input_tensor))\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVX_vxQSOHG0",
        "outputId": "0e3a9089-559d-48eb-9f39-4440b72fcaf1"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[17, 4, 2], [6, 3], [12, 5], [14, 3], [11, 2], [16], [7, 5, 2], [15, 6], [7, 3], [10, 13, 4, 9, 8]] \n",
            "\n",
            "tensor([[17,  4,  2,  1],\n",
            "        [ 6,  3,  1,  1],\n",
            "        [12,  5,  1,  1],\n",
            "        [14,  3,  1,  1],\n",
            "        [11,  2,  1,  1],\n",
            "        [16,  1,  1,  1],\n",
            "        [ 7,  5,  2,  1],\n",
            "        [15,  6,  1,  1],\n",
            "        [ 7,  3,  1,  1],\n",
            "        [10, 13,  4,  9]]) \n",
            "\n",
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "SimpleNN                                 [1, 1]                    --\n",
            "├─Embedding: 1-1                         [1, 4, 8]                 144\n",
            "├─Flatten: 1-2                           [1, 32]                   --\n",
            "├─Linear: 1-3                            [1, 1]                    33\n",
            "├─Sigmoid: 1-4                           [1, 1]                    --\n",
            "==========================================================================================\n",
            "Total params: 177\n",
            "Trainable params: 177\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 0.00\n",
            "==========================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "for epoch in range(num_epochs):\n",
        "    for padded_docs_batch, labels_batch in dataloader:\n",
        "        outputs = model(padded_docs_batch)\n",
        "        loss = criterion(outputs.squeeze(), labels_batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# eval\n",
        "with torch.no_grad():\n",
        "    outputs = model(padded_docs).squeeze()\n",
        "    predicted = (outputs >= 0.5).float()\n",
        "    accuracy = (predicted == labels).float().mean()\n",
        "    print('Accuracy: %f' % (accuracy.item() * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJURZa7wD9o6",
        "outputId": "c913bcbb-d308-40b8-b901-6f9f3cec4250"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v93tMfSpMx9F"
      },
      "execution_count": 48,
      "outputs": []
    }
  ]
}